from util.tokenizer import Tokenizer

model = Tokenizer()
print(model)

words = model.tokenize_de()
print(words)